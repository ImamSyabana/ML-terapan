# -*- coding: utf-8 -*-
"""FinalSubmisi_MLterapan.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Da_5SAI82tLLvs89pXKaHu0SdE5k-tcG

Proyek akhir Machine Learning Terapan : Muhammad Imam Ariq Sya'bana

# Data Understanding

### Data Wrangling
"""

from google.colab import drive
drive.mount('/content/drive')

"""Menyaipkan semua library yang dibutuhkan"""

#masukan librari
import numpy as np
import pandas as pd
import scipy
import matplotlib.pyplot as plt
import seaborn

"""### Gathering Data

Research Paper datasets : https://www.kaggle.com/datasets/vijendersingh412/research-paper

Datasets Link from Gdrive : https://drive.google.com/file/d/1JuU_lDvfr8XzkUCSwCl7yOqSWxWvMFMe/view?usp=sharing
"""

dataset = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/bangkit/Dicoding optional courses/ML Terapan/finalSubmission/dataset/Meta_research_paper_with_content.csv')

"""memvisualisasikan dataframe"""

dataset

"""### Assessing Data

mendeskripsikan metadata dari dataset yang digunakan
"""

print("Jumlah data Sub-Domain Paper : ", len(dataset['Sub-Domain'].unique()))

print("Jumlah data Judul Paper : ", len(dataset['Paper'].unique()))

print("Jumlah data Author Paper : ", len(dataset['Authors'].unique()))

print("Jumlah data Domain Paper : ", len(dataset['Domain'].unique()))

print("Jumlah data tautan Paper : ", len(dataset['Link'].unique()))

"""Memahami atribut-atribut yang ada di dalam dataset tersebut dilakukan dengan beberapa langkah untuk memahami isi dan tipe atribut tersebut."""

dataset.info()

"""Menghitung data yang unik pada masing-masing fitur untuk mengetahui seberapa luas distribusi koleksi paper yang dikumpulkan pada dataset ini."""

print("Jumlah paper : ", len(dataset.Paper.unique()))
print("\nContoh paper di dataset : ", dataset['Paper'][2])

print("Jumlah tautan ke masing-masing paper : ", len(dataset.Paper.unique()))
print("\nContoh tautan paper : ", dataset['Link'][2])

print("Jumlah Authors : ", len(dataset.Authors.unique()))
print("\nContoh Authors : ", dataset['Authors'][2])

"""authors hanya terdapat 81, sementara jumlah paper keseluruhan ada 82. Ini berarti ada satu authors yang memiliki dua judul paper"""

print("Jumlah Domain : ", len(dataset.Domain.unique()))
print("\nContoh Domain : ", dataset['Domain'][2])

"""domain pada datasets hanya berisi paper yang membicarakan tentang NLP"""

print("Jumlah Sub-Domain : ", len(dataset['Sub-Domain'].unique()))
print("\nSub-Domain : ", dataset['Sub-Domain'].unique())

"""memeriksa apakah ada data yang memiliki missing value"""

# Cek missing value dengan fungsi isnull()
dataset.isnull().sum()

"""karena tidak terdapat missing value, sehingga tidak perlu dilanjutkan ke tahap penanganan missing value seperti drop missing value, imputation, dan interpolation"""

# # Membersihkan missing value dengan fungsi dropna()
# dataset = dataset.dropna()
# dataset

"""mengecek apakah terdapat dua atau lebih data yang memiliki nilai yang sama pada semua atributnya (mengecek duplikat)"""

print("Jumlah duplikasi: ", dataset.duplicated().sum())

"""karena tidak terdapat nilai duplicate, sehingga tidak perlu dilanjutkan ke tahap penanganan duplicate value seperti drop_duplicates"""

# # Membuang data duplikat pada variabel preparation
# dataset = dataset.drop_duplicates()
# dataset

dataset.sample(5)

"""# Data Preprocessing

melakukan menganalisis data dan memanipulasi data

Memetakan semua atribut yang ada di dataset ke dalam variabelnya masing-masing
"""

index_paper = dataset.iloc[:,0].values # nomor index paper
paper_title = dataset.iloc[:, 1].values # judul paper
link = dataset.iloc[:, 2].values # tautan paper
authors = dataset.iloc[:,3].values # penulis paper
domain = dataset.iloc[:,4].values # domain paper
subdomain = dataset.iloc[:, -1].values # subdomain paper

type(paper_title)

index_paper[:5]

paper_title[:5]

link[:5]

authors[:5]

domain[-5:]

subdomain[-5:]

# docs_key = dataset.iloc[:, 0].values  # d1, d2, d3, etc
# docs = dataset.iloc[: , -2].values #untuk yang feature / independent
# label = dataset.iloc[: , -1].values # kategori dari document

"""Mengubah tipe data variabel masing-masing atribut dari numpy.ndarray menjadi list"""

index_paper_list = [] # rubah dari numpy array ke list
for i in range(len(index_paper)):
  index_paper_list.append(index_paper[i])

paper_title_list = [] # rubah dari numpy array ke list
for i in range(len(paper_title)):
  paper_title_list.append(paper_title[i])

link_list = [] # rubah dari numpy array ke list
for i in range(len(link)):
  link_list.append(link[i])

authors_list = [] # rubah dari numpy array ke list
for i in range(len(authors)):
  authors_list.append(authors[i])

subdomain_list = [] # rubah dari numpy array ke list
for i in range(len(subdomain)):
  subdomain_list.append(subdomain[i])

domain_list = [] # rubah dari numpy array ke list
for i in range(len(domain)):
  domain_list.append(domain[i])

index_paper_list, paper_title_list, link_list, authors_list, domain_list, subdomain_list

paper_title_list[0]

print(type(paper_title_list))

"""# Modelling

Metode yang digunakan untuk tahap pembuatan model machine learning untuk projek research paper recommendation adalah cosine similarity.

## mamasukkan querry yang ingin dicari
"""

query = input("Masukkan querry dari judul paper yang ingin dicari : ")

"""## Melakukan Cosine similarity"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


# preprocessing dan vektorisasi documents
vectorizer = TfidfVectorizer()
vectorized_docs = vectorizer.fit_transform(paper_title_list)


# preprocessing dan vektorisasi querry
query_vector = vectorizer.transform([query])

# Calculate cosine similarity between the query and each document
similarity_scores = cosine_similarity(query_vector, vectorized_docs)

# Print the similarity scores for each document

for doc_idx, score in enumerate(similarity_scores[0]):
    paper = paper_title_list[doc_idx]
    link  = link_list[doc_idx]
    author = authors_list[doc_idx]
    domain = domain_list[doc_idx]
    subdomain = subdomain_list[doc_idx]
    print(f"Similarity dengan paper dengan index {doc_idx+1}: {score:.5f}\nJudul Paper : {paper} \nLink : {link} \nPenulis : {author} \nDomain paper : {domain} \nSub-domain paper : {subdomain} \n\n")

query

similarity_scores[0]

"""## menyimpulkan kategori dari querry yang dimasukkan"""

len(paper_title_list)

# dictionary of lists
dict = {'index paper': index_paper_list, 'Judul paper' : paper_title_list, 'tautan paper':link_list, 'penulis paper' : authors_list, 'domain paper' : domain_list, 'sub-domain paper': subdomain_list, 'nilai kesamaan': similarity_scores[0]}

df_paper = pd.DataFrame(dict)
df_paper = df_paper[df_paper['nilai kesamaan'] != 0]
paper_sorted = df_paper.sort_values(by=['nilai kesamaan'], ascending=False)

paper_sorted[:5]

type(paper_sorted)

"""menampilkan semua rekomendasi dari sistem"""

(paper_sorted)

# Export the DataFrame to a CSV file
csv_filename = 'output_reccomendation.csv'
paper_sorted[:5].to_csv(csv_filename, index=False, encoding='utf-8', header=True)



"""# Evaluation

Melakukan evaluasi model rekomendasi dengan menggunakan metrics Precision, accuracy, Recall, dan F1-Score.
"""

print(query)

# Assuming you have a DataFrame named paper_sorted
pd.set_option('display.max_colwidth', None)  # Set maximum column width to display entire content

print(paper_sorted['Judul paper'])

len(paper_sorted['Judul paper'])

"""jika judul paper direkomendasikan dengan nilai kesamaan berapapun artinya paper tersebut ditebak oleh sistem sebagai dokumen yang relevan. Maka dari itu penulis menganggap bahwa dokumen tersebut adalah betul dokumen relevan.

mengubah dokumen yang memiliki nilai kesamaan dengan queri dari pengguna menjadi tebakan dokumen relevan sehingga bernilai 1, sedangkan yang tidak direkomendasikan akan diberi nilai 0
"""

predicted_labels = []

for x in range(len(similarity_scores[0])):
  if similarity_scores[0][x] != 0:
    predicted_labels.append(1)
  else:
    predicted_labels.append(0)

predicted_labels[:4]

predicted_labels_cnt = predicted_labels.count(1)
predicted_labels_cnt

"""mengecek apakan dokumen yang direkomendasikan oleh sistem relevan, sekaligus memastikan apakah ada dokumen yang relevan yang tidak direkomendasikan oleh sistem

Link	Authors	Domain	Sub-Domain
"""

true_labels = []

for x in range(len(dataset)):
  print("Nomor jurnal : ", dataset["Sno"][x])
  print("Judul Paper : ", dataset["Pap


  er"][x])
  print("Penulis paper : ", dataset["Authors"][x])
  print("Domain paper : ", dataset["Domain"][x])
  print("Subdomain paper : ", dataset["Sub-Domain"][x])

  print("\n")
  print("Queri dari user : ", query)
  print("Nilai kesamaan paper dengan queri : ", similarity_scores[0][x])
  print("\n")

  # melakukan kroscek untuk relevansi dokumen untuk mengevaluasi model
  true_relevance = input("Masukkan '1' untuk relevan dengan queri, input '0' untuk tidak relevan dengan queri : ")
  print("\n\n\n")
  true_labels.append(true_relevance)

true_labels[:20]

for x in range(len(true_labels)):
  true_labels[x] = int(true_labels[x])

true_labels[:20]

len(true_labels)

len(predicted_labels)

from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score


# mengkomputasi nilai precision, recall, F1-Score, dan akurasi
precision = precision_score(true_labels, predicted_labels)
recall = recall_score(true_labels, predicted_labels)
f1 = f1_score(true_labels, predicted_labels)
accuracy = accuracy_score(true_labels, predicted_labels)

# Menampilkan hasil
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")
print(f"Accuracy: {accuracy:.4f}")

